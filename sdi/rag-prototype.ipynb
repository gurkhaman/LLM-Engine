{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import tomllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\"../.env\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to receive requirements (markdown file) path and return the string\n",
    "\n",
    "def load_md_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file at {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred while reading the file: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Project Overview\\n- **Project Name**: Autonomous Turtlebot Navigation\\n- **Objective**: Enable Turtlebot to navigate autonomously while recognizing and responding to environmental cues.\\n- **Domain**: Mobility and Autonomous Vehicles\\n\\n# System Overview\\n- **System Context**: \\n  - Operating Environment: Indoor and Outdoor\\n  - OS: Ubuntu 20.04 LTS Focal Foss\\n  - Middleware: ROS2 Foxy Fitzroy\\n  - Key Components:\\n    - Camera: Captures images and publishes them to `/camera/image_raw`\\n    - Motion Controller: Receives commands via `/cmd_vel`\\n\\n# Functional Requirements\\n\\n## FR-001: Emergency Vehicle Recognition\\n- **Description**: The system shall detect and recognize emergency vehicles.\\n- **Input**: Image stream from `/camera/image_raw`.\\n- **Output**: Classification of the detected vehicle as an emergency vehicle with a confidence level above 95%.\\n\\n## FR-002: Path Alteration\\n- **Description**: Upon detecting an emergency vehicle, the Turtlebot shall automatically alter its path.\\n- **Input**: Detection result from the emergency vehicle recognition module.\\n- **Output**: Commands sent to the motion controller via `/cmd_vel`.\\n\\n# Non-Functional Requirements\\n\\n## NFR-001: Processing Time\\n- **Description**: The system shall process and react within 300ms.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirements = load_md_file(\"/workspaces/composition-blueprint-engine/sdi/requirements/turtlebot-emergency.md\")\n",
    "requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_markdown(directory):\n",
    "    documents = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".md\"):\n",
    "            md_path = os.path.join(directory, filename)\n",
    "            loader = UnstructuredMarkdownLoader(md_path)\n",
    "            data = loader.load()\n",
    "            if data and isinstance(data[0], Document):\n",
    "                documents.append(data[0])\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/yolov10n.md'}, page_content='license: agpl-3.0 tags: - object-detection - computer-vision - yolov10 - pytorch_model_hub_mixin datasets: - detection-datasets/coco library_name: yolov10 inference: false\\n\\nModel Description\\n\\nYOLOv10: Real-Time End-to-End Object Detection\\n\\narXiv: https://arxiv.org/abs/2405.14458v1\\n\\ngithub: https://github.com/THU-MIG/yolov10\\n\\nInstallation\\n\\npip install git+https://github.com/THU-MIG/yolov10.git\\n\\nTraining and validation\\n\\n```python from ultralytics import YOLOv10\\n\\nmodel = YOLOv10.from_pretrained(\\'jameslahm/yolov10n\\')\\n\\nTraining\\n\\nmodel.train(...)\\n\\nafter training, one can push to the hub\\n\\nmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")\\n\\nValidation\\n\\nmodel.val(...) ```\\n\\nInference\\n\\nHere\\'s an end-to-end example showcasing inference on a cats image:\\n\\n```python from ultralytics import YOLOv10\\n\\nmodel = YOLOv10.from_pretrained(\\'jameslahm/yolov10n\\') source = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\' model.predict(source=source, save=True) ``` which shows:\\n\\nBibTeX Entry and Citation Info\\n\\n@article{wang2024yolov10, title={YOLOv10: Real-Time End-to-End Object Detection}, author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang}, journal={arXiv preprint arXiv:2405.14458}, year={2024} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/gender-classification.md'}, page_content='tags: - image-classification - pytorch - huggingpics metrics: - accuracy\\n\\nmodel-index: - name: gender-classification results: - task: name: Image Classification type: image-classification metrics: - name: Accuracy type: accuracy value: 0.9244444370269775\\n\\ngender-classification\\n\\nAutogenerated by HuggingPicsü§óüñºÔ∏è\\n\\nCreate your own image classifier for anything by running the demo on Google Colab.\\n\\nReport any issues with the demo at the github repo.\\n\\nExample Images\\n\\nfemale\\n\\nmale'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/food-category-classification-v2.md'}, page_content='tags: - vision - image-classification datasets: - Kaludi/food-category-classification-v2.0 widget: - src: https://www.foodandwine.com/thmb/gv06VNqj1uUJHGlw5e7IULwUmr8=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/2012-r-xl-vegetable-sandwich-with-dill-sauce-2000-0984c1b513ae4af396aee039afa5e38c.jpg example_title: Bread - src: https://cdn.britannica.com/34/176234-050-0E0C55C6/Glass-milk.jpg example_title: Dairy - src: https://images-gmi-pmc.edge-generalmills.com/7c1096c7-bfd0-4806-a794-1d3001fe0063.jpg example_title: Dessert - src: https://theheirloompantry.co/wp-content/uploads/2022/06/how-to-fry-eggs-perfectly-in-4-ways-the-heirloom-pantry.jpg example_title: Egg - src: https://www.mashed.com/img/gallery/the-real-reason-fried-foods-are-so-popular-right-now/l-intro-1650327494.jpg example_title: Fried Food - src: https://www.seriouseats.com/thmb/WzQz05gt5witRGeOYKTcTqfe1gs=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/butter-basted-pan-seared-steaks-recipe-hero-06-03b1131c58524be2bd6c9851a2fbdbc3.jpg example_title: Meat - src: https://assets3.thrillist.com/v1/image/3097381/1200x600/scale; example_title: Seafood - src: https://i0.wp.com/post.healthline.com/wp-content/uploads/2020/03/romaine-lettuce-1296x728-body.jpg?w=1155&h=1528 example_title: Vegetable co2_eq_emissions: emissions: 12.456278925446485\\n\\nFood Category Classification v2.0\\n\\nThis is an updated Food Category Image Classifier model of the old model that has been trained by Kaludi to recognize 12 different categories of foods, which includes Bread, Dairy, Dessert, Egg, Fried Food, Fruit, Meat, Noodles, Rice, Seafood, Soup, and Vegetable. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.\\n\\nGradio\\n\\nThis model supports a Gradio Web UI to run the data-food-classification model:\\n\\nValidation Metrics\\n\\nProblem type: Multi-class Classification\\n\\nModel ID: 3353292434\\n\\nCO2 Emissions (in grams): 12.4563\\n\\nLoss: 0.144\\n\\nAccuracy: 0.960\\n\\nMacro F1: 0.959\\n\\nMicro F1: 0.960\\n\\nWeighted F1: 0.959\\n\\nMacro Precision: 0.962\\n\\nMicro Precision: 0.960\\n\\nWeighted Precision: 0.962\\n\\nMacro Recall: 0.960\\n\\nMicro Recall: 0.960\\n\\nWeighted Recall: 0.960'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/sd-image-variations-diffusers.md'}, page_content='thumbnail: \"https://repository-images.githubusercontent.com/523487884/fdb03a69-8353-4387-b5fc-0d85f888a63f\" datasets: - ChristophSchuhmann/improved_aesthetics_6plus license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - image-to-image\\n\\nStable Diffusion Image Variations Model Card\\n\\nüì£ V2 model released, and blurriness issues fixed! üì£\\n\\nüß®üéâ Image Variations is now natively supported in ü§ó Diffusers! üéâüß®\\n\\nVersion 2\\n\\nThis version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of \"image variations\" similar to DALLE-2 using Stable Diffusion. This version of the weights has been ported to huggingface Diffusers, to use this with the Diffusers library requires the Lambda Diffusers repo.\\n\\nThis model was trained in two stages and longer than the original variations model and gives better image quality and better CLIP rated similarity compared to the original version\\n\\nSee training details and v1 vs v2 comparison below.\\n\\nExample\\n\\nMake sure you are using a version of Diffusers >=0.8.0 (for older version see the old instructions at the bottom of this model card)\\n\\n```python from diffusers import StableDiffusionImageVariationPipeline from PIL import Image\\n\\ndevice = \"cuda:0\" sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained( \"lambdalabs/sd-image-variations-diffusers\", revision=\"v2.0\", ) sd_pipe = sd_pipe.to(device)\\n\\nim = Image.open(\"path/to/image.jpg\") tform = transforms.Compose([ transforms.ToTensor(), transforms.Resize( (224, 224), interpolation=transforms.InterpolationMode.BICUBIC, antialias=False, ), transforms.Normalize( [0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711]), ]) inp = tform(im).to(device).unsqueeze(0)\\n\\nout = sd_pipe(inp, guidance_scale=3) out[\"images\"][0].save(\"result.jpg\") ```\\n\\nThe importance of resizing correctly... (or not)\\n\\nNote that due a bit of an oversight during training, the model expects resized images without anti-aliasing. This turns out to make a big difference and is important to do the resizing the same way during inference. When passing a PIL image to the Diffusers pipeline antialiasing will be applied during resize, so it\\'s better to input a tensor which you have prepared manually according to the transfrom in the example above!\\n\\nHere are examples of images generated without (top) and with (bottom) anti-aliasing during resize. (Input is this image)\\n\\nV1 vs V2\\n\\nHere\\'s an example of V1 vs V2, version two was trained more carefully and for longer, see the details below. V2-top vs V1-bottom\\n\\nInput images:\\n\\nOne important thing to note is that due to the longer training V2 appears to have memorised some common images from the training data, e.g. now the previous example of the Girl with a Pearl Earring almosts perfectly reproduce the original rather than creating variations. You can always use v1 by specifiying revision=\"v1.0\".\\n\\nv2 output for girl with a pearl earing as input (guidance scale=3)\\n\\nTraining\\n\\nTraining Procedure This model is fine tuned from Stable Diffusion v1-3 where the text encoder has been replaced with an image encoder. The training procedure is the same as for Stable Diffusion except for the fact that images are encoded through a ViT-L/14 image-encoder including the final projection layer to the CLIP shared embedding space. The model was trained on LAION improved aesthetics 6plus.\\n\\nHardware: 8 x A100-40GB GPUs (provided by Lambda GPU Cloud)\\n\\nOptimizer: AdamW\\n\\nStage 1 - Fine tune only CrossAttention layer weights from Stable Diffusion v1.4 model\\n\\nSteps: 46,000\\n\\nBatch: batch size=4, GPUs=8, Gradient Accumulations=4. Total batch size=128\\n\\nLearning rate: warmup to 1e-5 for 10,000 steps and then kept constant\\n\\nStage 2 - Resume from Stage 1 training the whole unet\\n\\nSteps: 50,000\\n\\nBatch: batch size=4, GPUs=8, Gradient Accumulations=5. Total batch size=160\\n\\nLearning rate: warmup to 1e-5 for 5,000 steps and then kept constant\\n\\nTraining was done using a modified version of the original Stable Diffusion training code.\\n\\nUses\\n\\nThe following section is adapted from the Stable Diffusion model card\\n\\nDirect Use\\n\\nThe model is intended for research purposes only. Possible research areas and tasks include\\n\\nSafe deployment of models which have the potential to generate harmful content.\\n\\nProbing and understanding the limitations and biases of generative models.\\n\\nGeneration of artworks and use in design and other artistic processes.\\n\\nApplications in educational or creative tools.\\n\\nResearch on generative models.\\n\\nExcluded uses are described below.\\n\\n### Misuse, Malicious Use, and Out-of-Scope Use\\n\\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\\n\\nOut-of-Scope Use\\n\\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\nMisuse and Malicious Use\\n\\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n\\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\\n\\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\\n\\nImpersonating individuals without their consent.\\n\\nSexual content without consent of the people who might see it.\\n\\nMis- and disinformation\\n\\nRepresentations of egregious violence and gore\\n\\nSharing of copyrighted or licensed material in violation of its terms of use.\\n\\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\\n\\nLimitations and Bias\\n\\nLimitations\\n\\nThe model does not achieve perfect photorealism\\n\\nThe model cannot render legible text\\n\\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\\n\\nFaces and people in general may not be generated properly.\\n\\nThe model was trained mainly with English captions and will not work as well in other languages.\\n\\nThe autoencoding part of the model is lossy\\n\\nThe model was trained on a large-scale dataset LAION-5B which contains adult material and is not fit for product use without additional safety mechanisms and considerations.\\n\\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data. The training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\\n\\nBias\\n\\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion v1 was trained on subsets of LAION-2B(en), which consists of images that are primarily limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\\n\\nSafety Module\\n\\nThe intended use of this model is with the Safety Checker in Diffusers. This checker works by checking model outputs against known hard-coded NSFW concepts. The concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter. Specifically, the checker compares the class probability of harmful concepts in the embedding space of the CLIPModel after generation of the images. The concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\\n\\nOld instructions\\n\\nIf you are using a diffusers version <0.8.0 there is no StableDiffusionImageVariationPipeline, in this case you need to use an older revision (2ddbd90b14bc5892c19925b15185e561bc8e5d0a) in conjunction with the lambda-diffusers repo:\\n\\nFirst clone Lambda Diffusers and install any requirements (in a virtual environment in the example below):\\n\\nbash git clone https://github.com/LambdaLabsML/lambda-diffusers.git cd lambda-diffusers python -m venv .venv source .venv/bin/activate pip install -r requirements.txt\\n\\nThen run the following python code:\\n\\n```python from pathlib import Path from lambda_diffusers import StableDiffusionImageEmbedPipeline from PIL import Image import torch\\n\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" pipe = StableDiffusionImageEmbedPipeline.from_pretrained( \"lambdalabs/sd-image-variations-diffusers\", revision=\"2ddbd90b14bc5892c19925b15185e561bc8e5d0a\", ) pipe = pipe.to(device)\\n\\nim = Image.open(\"your/input/image/here.jpg\") num_samples = 4 image = pipe(num_samples*[im], guidance_scale=3.0) image = image[\"sample\"]\\n\\nbase_path = Path(\"outputs/im2im\") base_path.mkdir(exist_ok=True, parents=True) for idx, im in enumerate(image): im.save(base_path/f\"{idx:06}.jpg\") ```\\n\\nThis model card was written by: Justin Pinkney and is based on the Stable Diffusion model card.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/resnet-50.md'}, page_content='license: apache-2.0 tags: - vision - image-classification datasets: - imagenet-1k\\n\\nResNet-50 v1.5\\n\\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al.\\n\\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\\n\\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (\\\\~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\\n\\n```python from transformers import AutoImageProcessor, ResNetForImageClassification import torch from datasets import load_dataset\\n\\ndataset = load_dataset(\"huggingface/cats-image\") image = dataset[\"test\"][\"image\"][0]\\n\\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\") model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\\n\\ninputs = processor(image, return_tensors=\"pt\")\\n\\nwith torch.no_grad(): logits = model(**inputs).logits\\n\\nmodel predicts one of the 1000 ImageNet classes\\n\\npredicted_label = logits.argmax(-1).item() print(model.config.id2label[predicted_label]) ```\\n\\nFor more code examples, we refer to the documentation.\\n\\nBibTeX entry and citation info\\n\\nbibtex @inproceedings{he2016deep, title={Deep residual learning for image recognition}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition}, pages={770--778}, year={2016} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/yolos-tiny.md'}, page_content='license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport\\n\\nYOLOS (tiny-sized) model\\n\\nYOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository.\\n\\nDisclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nYOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\\n\\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for object detection. See the model hub to look for all available YOLOS models.\\n\\nHow to use\\n\\nHere is how to use this model:\\n\\n```python from transformers import YolosImageProcessor, YolosForObjectDetection from PIL import Image import torch import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nmodel = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-tiny\\') image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\\n\\ninputs = image_processor(images=image, return_tensors=\"pt\") outputs = model(**inputs)\\n\\nmodel predicts bounding boxes and corresponding COCO classes\\n\\nlogits = outputs.logits bboxes = outputs.pred_boxes\\n\\nprint results\\n\\ntarget_sizes = torch.tensor([image.size[::-1]]) results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0] for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]): box = [round(i, 2) for i in box.tolist()] print( f\"Detected {model.config.id2label[label.item()]} with confidence \" f\"{round(score.item(), 3)} at location {box}\" ) ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe YOLOS model was pre-trained on ImageNet-1k and fine-tuned on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\\n\\nTraining\\n\\nThe model was pre-trained for 300 epochs on ImageNet-1k and fine-tuned for 300 epochs on COCO.\\n\\nEvaluation results\\n\\nThis model achieves an AP (average precision) of 28.7 on COCO 2017 validation. For more details regarding evaluation results, we refer to the original paper.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2106-00666, author = {Yuxin Fang and Bencheng Liao and Xinggang Wang and Jiemin Fang and Jiyang Qi and Rui Wu and Jianwei Niu and Wenyu Liu}, title = {You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection}, journal = {CoRR}, volume = {abs/2106.00666}, year = {2021}, url = {https://arxiv.org/abs/2106.00666}, eprinttype = {arXiv}, eprint = {2106.00666}, timestamp = {Fri, 29 Apr 2022 19:49:16 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2106-00666.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/yolos-small.md'}, page_content='license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport\\n\\nYOLOS (small-sized) model\\n\\nYOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository.\\n\\nDisclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nYOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\\n\\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for object detection. See the model hub to look for all available YOLOS models.\\n\\nHow to use\\n\\nHere is how to use this model:\\n\\n```python from transformers import YolosFeatureExtractor, YolosForObjectDetection from PIL import Image import requests\\n\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\' image = Image.open(requests.get(url, stream=True).raw)\\n\\nfeature_extractor = YolosFeatureExtractor.from_pretrained(\\'hustvl/yolos-small\\') model = YolosForObjectDetection.from_pretrained(\\'hustvl/yolos-small\\')\\n\\ninputs = feature_extractor(images=image, return_tensors=\"pt\") outputs = model(**inputs)\\n\\nmodel predicts bounding boxes and corresponding COCO classes\\n\\nlogits = outputs.logits bboxes = outputs.pred_boxes ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe YOLOS model was pre-trained on ImageNet-1k and fine-tuned on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\\n\\nTraining\\n\\nThe model was pre-trained for 200 epochs on ImageNet-1k and fine-tuned for 150 epochs on COCO.\\n\\nEvaluation results\\n\\nThis model achieves an AP (average precision) of 36.1 on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2106-00666, author = {Yuxin Fang and Bencheng Liao and Xinggang Wang and Jiemin Fang and Jiyang Qi and Rui Wu and Jianwei Niu and Wenyu Liu}, title = {You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection}, journal = {CoRR}, volume = {abs/2106.00666}, year = {2021}, url = {https://arxiv.org/abs/2106.00666}, eprinttype = {arXiv}, eprint = {2106.00666}, timestamp = {Fri, 29 Apr 2022 19:49:16 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2106-00666.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/detr-resnet-101.md'}, page_content='license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport\\n\\nDETR (End-to-End Object Detection) model with ResNet-101 backbone\\n\\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\\n\\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.\\n\\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for object detection. See the model hub to look for all available DETR models.\\n\\nHow to use\\n\\nHere is how to use this model:\\n\\n```python from transformers import DetrImageProcessor, DetrForObjectDetection import torch from PIL import Image import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nyou can specify the revision tag if you don\\'t want the timm dependency\\n\\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\") model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\", revision=\"no_timm\")\\n\\ninputs = processor(images=image, return_tensors=\"pt\") outputs = model(**inputs)\\n\\nconvert outputs (bounding boxes and class logits) to COCO API\\n\\nlet\\'s only keep detections with score > 0.9\\n\\ntarget_sizes = torch.tensor([image.size[::-1]]) results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\n\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]): box = [round(i, 2) for i in box.tolist()] print( f\"Detected {model.config.id2label[label.item()]} with confidence \" f\"{round(score.item(), 3)} at location {box}\" ) This should output (something along the lines of): Detected cat with confidence 0.998 at location [344.06, 24.85, 640.34, 373.74] Detected remote with confidence 0.997 at location [328.13, 75.93, 372.81, 187.66] Detected remote with confidence 0.997 at location [39.34, 70.13, 175.56, 118.78] Detected cat with confidence 0.998 at location [15.36, 51.75, 316.89, 471.16] Detected couch with confidence 0.995 at location [-0.19, 0.71, 639.73, 474.17] ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe DETR model was trained on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\\n\\nTraining procedure\\n\\nPreprocessing\\n\\nThe exact details of preprocessing of images during training/validation can be found here.\\n\\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\\n\\nTraining\\n\\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\\n\\nEvaluation results\\n\\nThis model achieves an AP (average precision) of 43.5 on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2005-12872, author = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko}, title = {End-to-End Object Detection with Transformers}, journal = {CoRR}, volume = {abs/2005.12872}, year = {2020}, url = {https://arxiv.org/abs/2005.12872}, archivePrefix = {arXiv}, eprint = {2005.12872}, timestamp = {Thu, 28 May 2020 17:38:09 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/rtdetr_r18vd.md'}, page_content='library_name: transformers license: apache-2.0 language: - en pipeline_tag: object-detection tags: - object-detection - vision datasets: - coco widget: - src: >- https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: >- https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: >- https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport\\n\\nModel Card for RT-DETR\\n\\nTable of Contents\\n\\nModel Details\\n\\nModel Sources\\n\\nHow to Get Started with the Model\\n\\nTraining Details\\n\\nEvaluation\\n\\nModel Architecture and Objective\\n\\nCitation\\n\\nModel Details\\n\\nThe YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: this https URL.\\n\\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub.\\n\\nDeveloped by: Yian Zhao and Sangbum Choi\\n\\nFunded by: National Key R&D Program of China (No.2022ZD0118201), Natural Science Foundation of China (No.61972217, 32071459, 62176249, 62006133, 62271465), and the Shenzhen Medical Research Funds in China (No. B2302037).\\n\\nShared by: Sangbum Choi\\n\\nModel type: RT-DETR\\n\\nLicense: Apache-2.0\\n\\nModel Sources\\n\\nHF Docs: RT-DETR\\n\\nRepository: https://github.com/lyuwenyu/RT-DETR\\n\\nPaper: https://arxiv.org/abs/2304.08069\\n\\nDemo: RT-DETR Tracking\\n\\nHow to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n```python import torch import requests\\n\\nfrom PIL import Image from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\\n\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\' image = Image.open(requests.get(url, stream=True).raw)\\n\\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r18vd\") model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r18vd\")\\n\\ninputs = image_processor(images=image, return_tensors=\"pt\")\\n\\nwith torch.no_grad(): outputs = model(**inputs)\\n\\nresults = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\\n\\nfor result in results: for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]): score, label = score.item(), label_id.item() box = [round(i, 2) for i in box.tolist()] print(f\"{model.config.id2label[label]}: {score:.2f} {box}\") This should output sofa: 0.97 [0.14, 0.38, 640.13, 476.21] cat: 0.96 [343.38, 24.28, 640.14, 371.5] cat: 0.96 [13.23, 54.18, 318.98, 472.22] remote: 0.95 [40.11, 73.44, 175.96, 118.48] remote: 0.92 [333.73, 76.58, 369.97, 186.99] ```\\n\\nTraining Details\\n\\nTraining Data\\n\\nThe RTDETR model was trained on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\\n\\nTraining Procedure\\n\\nWe conduct experiments on COCO and Objects365 datasets, where RT-DETR is trained on COCO train2017 and validated on COCO val2017 dataset. We report the standard COCO metrics, including AP (averaged over uniformly sampled IoU thresholds ranging from 0.50-0.95 with a step size of 0.05), AP50, AP75, as well as AP at different scales: APS, APM, APL.\\n\\nPreprocessing\\n\\nImages are resized to 640x640 pixels and rescaled with image_mean=[0.485, 0.456, 0.406] and image_std=[0.229, 0.224, 0.225].\\n\\nTraining Hyperparameters\\n\\nTraining regime:\\n\\nEvaluation\\n\\nModel #Epochs #Params (M) GFLOPs FPS_bs=1 AP (val) AP50 (val) AP75 (val) AP-s (val) AP-m (val) AP-l (val) RT-DETR-R18 72 20 60.7 217 46.5 63.8 50.4 28.4 49.8 63.0 RT-DETR-R34 72 31 91.0 172 48.5 66.2 52.3 30.2 51.9 66.2 RT-DETR R50 72 42 136 108 53.1 71.3 57.7 34.8 58.0 70.0 RT-DETR R101 72 76 259 74 54.3 72.7 58.6 36.0 58.8 72.1 RT-DETR-R18 (Objects 365 pretrained) 60 20 61 217 49.2 66.6 53.5 33.2 52.3 64.8 RT-DETR-R50 (Objects 365 pretrained) 24 42 136 108 55.3 73.4 60.1 37.9 59.9 71.8 RT-DETR-R101 (Objects 365 pretrained) 24 76 259 74 56.2 74.6 61.3 38.3 60.5 73.5\\n\\nModel Architecture and Objective\\n\\nOverview of RT-DETR. We feed the features from the last three stages of the backbone into the encoder. The efficient hybrid encoder transforms multi-scale features into a sequence of image features through the Attention-based Intra-scale Feature Interaction (AIFI) and the CNN-based Cross-scale Feature Fusion (CCFF). Then, the uncertainty-minimal query selection selects a fixed number of encoder features to serve as initial object queries for the decoder. Finally, the decoder with auxiliary prediction heads iteratively optimizes object queries to generate categories and boxes.\\n\\nCitation\\n\\nBibTeX:\\n\\nbibtex @misc{lv2023detrs, title={DETRs Beat YOLOs on Real-time Object Detection}, author={Yian Zhao and Wenyu Lv and Shangliang Xu and Jinman Wei and Guanzhong Wang and Qingqing Dang and Yi Liu and Jie Chen}, year={2023}, eprint={2304.08069}, archivePrefix={arXiv}, primaryClass={cs.CV} }\\n\\nModel Card Authors\\n\\nSangbum Choi Pavel Iakubovskii'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/stable-diffusion-x4-upscaler.md'}, page_content='license: openrail++ tags: - stable-diffusion inference: false\\n\\nStable Diffusion x4 upscaler model card\\n\\nThis model card focuses on the model associated with the Stable Diffusion Upscaler, available here. This model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\\n\\nUse it with the stablediffusion repository: download the x4-upscaler-ema.ckpt here.\\n\\nUse it with üß® diffusers\\n\\nModel Details\\n\\nDeveloped by: Robin Rombach, Patrick Esser\\n\\nModel type: Diffusion-based text-to-image generation model\\n\\nLanguage(s): English\\n\\nLicense: CreativeML Open RAIL++-M License\\n\\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\\n\\nResources for more information: GitHub Repository.\\n\\nCite as:\\n\\n@InProceedings{Rombach_2022_CVPR, author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn}, title = {High-Resolution Image Synthesis With Latent Diffusion Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2022}, pages = {10684-10695} }\\n\\nExamples\\n\\nUsing the ü§ó\\'s Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\\n\\nbash pip install diffusers transformers accelerate scipy safetensors\\n\\n```python import requests from PIL import Image from io import BytesIO from diffusers import StableDiffusionUpscalePipeline import torch\\n\\nload model and scheduler\\n\\nmodel_id = \"stabilityai/stable-diffusion-x4-upscaler\" pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16) pipeline = pipeline.to(\"cuda\")\\n\\nlet\\'s download an image\\n\\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\" response = requests.get(url) low_res_img = Image.open(BytesIO(response.content)).convert(\"RGB\") low_res_img = low_res_img.resize((128, 128))\\n\\nprompt = \"a white cat\"\\n\\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0] upscaled_image.save(\"upsampled_cat.png\") ```\\n\\nNotes: - Despite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance) - If you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\\n\\nUses\\n\\nDirect Use\\n\\nThe model is intended for research purposes only. Possible research areas and tasks include\\n\\nSafe deployment of models which have the potential to generate harmful content.\\n\\nProbing and understanding the limitations and biases of generative models.\\n\\nGeneration of artworks and use in design and other artistic processes.\\n\\nApplications in educational or creative tools.\\n\\nResearch on generative models.\\n\\nExcluded uses are described below.\\n\\n### Misuse, Malicious Use, and Out-of-Scope Use Note: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\\n\\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\\n\\nOut-of-Scope Use\\n\\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\nMisuse and Malicious Use\\n\\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n\\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\\n\\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\\n\\nImpersonating individuals without their consent.\\n\\nSexual content without consent of the people who might see it.\\n\\nMis- and disinformation\\n\\nRepresentations of egregious violence and gore\\n\\nSharing of copyrighted or licensed material in violation of its terms of use.\\n\\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\\n\\nLimitations and Bias\\n\\nLimitations\\n\\nThe model does not achieve perfect photorealism\\n\\nThe model cannot render legible text\\n\\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\\n\\nFaces and people in general may not be generated properly.\\n\\nThe model was trained mainly with English captions and will not work as well in other languages.\\n\\nThe autoencoding part of the model is lossy\\n\\nThe model was trained on a subset of the large-scale dataset LAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION\\'s NFSW detector (see Training section).\\n\\nBias\\n\\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion vw was primarily trained on subsets of LAION-2B(en), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\\n\\nTraining\\n\\nTraining Data The model developers used the following dataset for training the model:\\n\\nLAION-5B and subsets (details below). The training data is further filtered using LAION\\'s NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B\\'s NeurIPS 2022 paper and reviewer discussions on the topic.\\n\\nTraining Procedure Stable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\\n\\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\\n\\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\\n\\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\\n\\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\\n\\nWe currently provide the following checkpoints:\\n\\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. 850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\\n\\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\\n\\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized.\\n\\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\\n\\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\\n\\nHardware: 32 x 8 x A100 GPUs\\n\\nOptimizer: AdamW\\n\\nGradient Accumulations: 1\\n\\nBatch: 32 x 8 x 2 x 4 = 2048\\n\\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\\n\\nEvaluation Results\\n\\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\\n\\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.\\n\\nEnvironmental Impact\\n\\nStable Diffusion v1 Estimated Emissions Based on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\\n\\nHardware Type: A100 PCIe 40GB\\n\\nHours used: 200000\\n\\nCloud Provider: AWS\\n\\nCompute Region: US-east\\n\\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\\n\\nCitation\\n\\n@InProceedings{Rombach_2022_CVPR,\\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn},\\n    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n    month     = {June},\\n    year      = {2022},\\n    pages     = {10684-10695}\\n}\\n\\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/stable-diffusion-2-inpainting.md'}, page_content='license: openrail++ tags: - stable-diffusion inference: false\\n\\nStable Diffusion v2 Model Card\\n\\nThis model card focuses on the model associated with the Stable Diffusion v2, available here.\\n\\nThis stable-diffusion-2-inpainting model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\\n\\nUse it with the stablediffusion repository: download the 512-inpainting-ema.ckpt here.\\n\\nUse it with üß® diffusers\\n\\nModel Details\\n\\nDeveloped by: Robin Rombach, Patrick Esser\\n\\nModel type: Diffusion-based text-to-image generation model\\n\\nLanguage(s): English\\n\\nLicense: CreativeML Open RAIL++-M License\\n\\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\\n\\nResources for more information: GitHub Repository.\\n\\nCite as:\\n\\n@InProceedings{Rombach_2022_CVPR, author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn}, title = {High-Resolution Image Synthesis With Latent Diffusion Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2022}, pages = {10684-10695} }\\n\\nExamples\\n\\nUsing the ü§ó\\'s Diffusers library to run Stable Diffusion 2 inpainting in a simple and efficient manner.\\n\\nbash pip install diffusers transformers accelerate scipy safetensors\\n\\n```python from diffusers import StableDiffusionInpaintPipeline pipe = StableDiffusionInpaintPipeline.from_pretrained( \"stabilityai/stable-diffusion-2-inpainting\", torch_dtype=torch.float16, ) pipe.to(\"cuda\") prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\\n\\nimage and mask_image should be PIL images.\\n\\nThe mask structure is white for inpainting and black for keeping as is\\n\\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0] image.save(\"./yellow_cat_on_park_bench.png\") ```\\n\\nNotes: - Despite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance) - If you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\\n\\nHow it works: image | mask_image :-------------------------:|:-------------------------:| |\\n\\nprompt | Output :-------------------------:|:-------------------------:| Face of a yellow cat, high resolution, sitting on a park bench |\\n\\nUses\\n\\nDirect Use\\n\\nThe model is intended for research purposes only. Possible research areas and tasks include\\n\\nSafe deployment of models which have the potential to generate harmful content.\\n\\nProbing and understanding the limitations and biases of generative models.\\n\\nGeneration of artworks and use in design and other artistic processes.\\n\\nApplications in educational or creative tools.\\n\\nResearch on generative models.\\n\\nExcluded uses are described below.\\n\\n### Misuse, Malicious Use, and Out-of-Scope Use Note: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\\n\\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\\n\\nOut-of-Scope Use\\n\\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\nMisuse and Malicious Use\\n\\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n\\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\\n\\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\\n\\nImpersonating individuals without their consent.\\n\\nSexual content without consent of the people who might see it.\\n\\nMis- and disinformation\\n\\nRepresentations of egregious violence and gore\\n\\nSharing of copyrighted or licensed material in violation of its terms of use.\\n\\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\\n\\nLimitations and Bias\\n\\nLimitations\\n\\nThe model does not achieve perfect photorealism\\n\\nThe model cannot render legible text\\n\\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\\n\\nFaces and people in general may not be generated properly.\\n\\nThe model was trained mainly with English captions and will not work as well in other languages.\\n\\nThe autoencoding part of the model is lossy\\n\\nThe model was trained on a subset of the large-scale dataset LAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION\\'s NFSW detector (see Training section).\\n\\nBias\\n\\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion vw was primarily trained on subsets of LAION-2B(en), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\\n\\nTraining\\n\\nTraining Data The model developers used the following dataset for training the model:\\n\\nLAION-5B and subsets (details below). The training data is further filtered using LAION\\'s NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B\\'s NeurIPS 2022 paper and reviewer discussions on the topic.\\n\\nTraining Procedure Stable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\\n\\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\\n\\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\\n\\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\\n\\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\\n\\nWe currently provide the following checkpoints:\\n\\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. 850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\\n\\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\\n\\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized.\\n\\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\\n\\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\\n\\nHardware: 32 x 8 x A100 GPUs\\n\\nOptimizer: AdamW\\n\\nGradient Accumulations: 1\\n\\nBatch: 32 x 8 x 2 x 4 = 2048\\n\\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\\n\\nEvaluation Results\\n\\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\\n\\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.\\n\\nEnvironmental Impact\\n\\nStable Diffusion v1 Estimated Emissions Based on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\\n\\nHardware Type: A100 PCIe 40GB\\n\\nHours used: 200000\\n\\nCloud Provider: AWS\\n\\nCompute Region: US-east\\n\\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\\n\\nCitation\\n\\n@InProceedings{Rombach_2022_CVPR,\\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn},\\n    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n    month     = {June},\\n    year      = {2022},\\n    pages     = {10684-10695}\\n}\\n\\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/swinv2-tiny-patch4-window16-256.md'}, page_content='license: apache-2.0 tags: - vision - image-classification datasets: - imagenet-1k widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace\\n\\nSwin Transformer v2 (tiny-sized model)\\n\\nSwin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository.\\n\\nDisclaimer: The team releasing Swin Transformer v2 did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nThe Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.\\n\\nSwin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\\n\\nSource\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\\n\\n```python from transformers import AutoImageProcessor, AutoModelForImageClassification from PIL import Image import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\") model = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\\n\\ninputs = processor(images=image, return_tensors=\"pt\") outputs = model(**inputs) logits = outputs.logits\\n\\nmodel predicts one of the 1000 ImageNet classes\\n\\npredicted_class_idx = logits.argmax(-1).item() print(\"Predicted class:\", model.config.id2label[predicted_class_idx]) ```\\n\\nFor more code examples, we refer to the documentation.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2111-09883, author = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo}, title = {Swin Transformer {V2:} Scaling Up Capacity and Resolution}, journal = {CoRR}, volume = {abs/2111.09883}, year = {2021}, url = {https://arxiv.org/abs/2111.09883}, eprinttype = {arXiv}, eprint = {2111.09883}, timestamp = {Thu, 02 Dec 2021 15:54:22 +0100}, biburl = {https://dblp.org/rec/journals/corr/abs-2111-09883.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/beit-base-patch16-224-pt22k-ft22k.md'}, page_content='license: apache-2.0 tags: - image-classification - vision datasets: - imagenet - imagenet-21k\\n\\nBEiT (base-sized model, fine-tuned on ImageNet-22k)\\n\\nBEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.\\n\\nDisclaimer: The team releasing BEiT did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nThe BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). In contrast to the original ViT model, BEiT is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. The pre-training objective for the model is to predict visual tokens from the encoder of OpenAI\\'s DALL-E\\'s VQ-VAE, based on masked patches. Next, the model was fine-tuned in a supervised fashion on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\\n\\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.\\n\\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image. Alternatively, one can mean-pool the final hidden states of the patch embeddings, and place a linear layer on top of that.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\\n\\n```python from transformers import BeitImageProcessor, BeitForImageClassification from PIL import Image import requests\\n\\nurl = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\' image = Image.open(requests.get(url, stream=True).raw)\\n\\nprocessor = BeitImageProcessor.from_pretrained(\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\') model = BeitForImageClassification.from_pretrained(\\'microsoft/beit-base-patch16-224-pt22k-ft22k\\')\\n\\ninputs = processor(images=image, return_tensors=\"pt\") outputs = model(**inputs) logits = outputs.logits\\n\\nmodel predicts one of the 21,841 ImageNet-22k classes\\n\\npredicted_class_idx = logits.argmax(-1).item() print(\"Predicted class:\", model.config.id2label[predicted_class_idx]) ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe BEiT model was pretrained on ImageNet-21k, a dataset consisting of 14 million images and 21k classes, and fine-tuned on the same dataset.\\n\\nTraining procedure\\n\\nPreprocessing\\n\\nThe exact details of preprocessing of images during training/validation can be found here.\\n\\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\\n\\nPretraining\\n\\nFor all pre-training related hyperparameters, we refer to page 15 of the original paper.\\n\\nEvaluation results\\n\\nFor evaluation results on several image classification benchmarks, we refer to tables 1 and 2 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution. Of course, increasing the model size will result in better performance.\\n\\nBibTeX entry and citation info\\n\\n@article{DBLP:journals/corr/abs-2106-08254, author = {Hangbo Bao and Li Dong and Furu Wei}, title = {BEiT: {BERT} Pre-Training of Image Transformers}, journal = {CoRR}, volume = {abs/2106.08254}, year = {2021}, url = {https://arxiv.org/abs/2106.08254}, archivePrefix = {arXiv}, eprint = {2106.08254}, timestamp = {Tue, 29 Jun 2021 16:55:04 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }\\n\\nbibtex @inproceedings{deng2009imagenet, title={Imagenet: A large-scale hierarchical image database}, author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}, booktitle={2009 IEEE conference on computer vision and pattern recognition}, pages={248--255}, year={2009}, organization={Ieee} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/stable-diffusion-2.md'}, page_content='license: openrail++ tags: - stable-diffusion - text-to-image\\n\\nStable Diffusion v2 Model Card\\n\\nThis model card focuses on the model associated with the Stable Diffusion v2 model, available here.\\n\\nThis stable-diffusion-2 model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on 768x768 images.\\n\\nUse it with the stablediffusion repository: download the 768-v-ema.ckpt here.\\n\\nUse it with üß® diffusers\\n\\nModel Details\\n\\nDeveloped by: Robin Rombach, Patrick Esser\\n\\nModel type: Diffusion-based text-to-image generation model\\n\\nLanguage(s): English\\n\\nLicense: CreativeML Open RAIL++-M License\\n\\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\\n\\nResources for more information: GitHub Repository.\\n\\nCite as:\\n\\n@InProceedings{Rombach_2022_CVPR, author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn}, title = {High-Resolution Image Synthesis With Latent Diffusion Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2022}, pages = {10684-10695} }\\n\\nExamples\\n\\nUsing the ü§ó\\'s Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\\n\\nbash pip install diffusers transformers accelerate scipy safetensors\\n\\nRunning the pipeline (if you don\\'t swap the scheduler it will run with the default DDIM, in this example we are swapping it to EulerDiscreteScheduler):\\n\\n```python from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\\n\\nmodel_id = \"stabilityai/stable-diffusion-2\"\\n\\nUse the Euler scheduler here instead\\n\\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\") pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16) pipe = pipe.to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\" image = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\") ```\\n\\nNotes: - Despite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance) - If you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\\n\\nUses\\n\\nDirect Use\\n\\nThe model is intended for research purposes only. Possible research areas and tasks include\\n\\nSafe deployment of models which have the potential to generate harmful content.\\n\\nProbing and understanding the limitations and biases of generative models.\\n\\nGeneration of artworks and use in design and other artistic processes.\\n\\nApplications in educational or creative tools.\\n\\nResearch on generative models.\\n\\nExcluded uses are described below.\\n\\n### Misuse, Malicious Use, and Out-of-Scope Use Note: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\\n\\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\\n\\nOut-of-Scope Use\\n\\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\nMisuse and Malicious Use\\n\\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n\\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\\n\\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\\n\\nImpersonating individuals without their consent.\\n\\nSexual content without consent of the people who might see it.\\n\\nMis- and disinformation\\n\\nRepresentations of egregious violence and gore\\n\\nSharing of copyrighted or licensed material in violation of its terms of use.\\n\\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\\n\\nLimitations and Bias\\n\\nLimitations\\n\\nThe model does not achieve perfect photorealism\\n\\nThe model cannot render legible text\\n\\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\\n\\nFaces and people in general may not be generated properly.\\n\\nThe model was trained mainly with English captions and will not work as well in other languages.\\n\\nThe autoencoding part of the model is lossy\\n\\nThe model was trained on a subset of the large-scale dataset LAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION\\'s NFSW detector (see Training section).\\n\\nBias\\n\\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion was primarily trained on subsets of LAION-2B(en), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\\n\\nTraining\\n\\nTraining Data The model developers used the following dataset for training the model:\\n\\nLAION-5B and subsets (details below). The training data is further filtered using LAION\\'s NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B\\'s NeurIPS 2022 paper and reviewer discussions on the topic.\\n\\nTraining Procedure Stable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\\n\\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\\n\\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\\n\\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\\n\\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\\n\\nWe currently provide the following checkpoints:\\n\\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. 850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\\n\\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\\n\\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized.\\n\\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning. The additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\\n\\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\\n\\nHardware: 32 x 8 x A100 GPUs\\n\\nOptimizer: AdamW\\n\\nGradient Accumulations: 1\\n\\nBatch: 32 x 8 x 2 x 4 = 2048\\n\\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\\n\\nEvaluation Results\\n\\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\\n\\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution. Not optimized for FID scores.\\n\\nEnvironmental Impact\\n\\nStable Diffusion v1 Estimated Emissions Based on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\\n\\nHardware Type: A100 PCIe 40GB\\n\\nHours used: 200000\\n\\nCloud Provider: AWS\\n\\nCompute Region: US-east\\n\\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\\n\\nCitation\\n\\n@InProceedings{Rombach_2022_CVPR,\\n    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\\\"orn},\\n    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n    month     = {June},\\n    year      = {2022},\\n    pages     = {10684-10695}\\n}\\n\\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/vit-face-expression.md'}, page_content=\"{}\\n\\nVision Transformer (ViT) for Facial Expression Recognition Model Card\\n\\nModel Overview\\n\\nModel Name: trpakov/vit-face-expression\\n\\nTask: Facial Expression/Emotion Recognition\\n\\nDataset: FER2013\\n\\nModel Architecture: Vision Transformer (ViT)\\n\\nFinetuned from model: vit-base-patch16-224-in21k\\n\\nModel Description\\n\\nThe vit-face-expression model is a Vision Transformer fine-tuned for the task of facial emotion recognition.\\n\\nIt is trained on the FER2013 dataset, which consists of facial images categorized into seven different emotions: - Angry - Disgust - Fear - Happy - Sad - Surprise - Neutral\\n\\nData Preprocessing\\n\\nThe input images are preprocessed before being fed into the model. The preprocessing steps include: - Resizing: Images are resized to the specified input size. - Normalization: Pixel values are normalized to a specific range. - Data Augmentation: Random transformations such as rotations, flips, and zooms are applied to augment the training dataset.\\n\\nEvaluation Metrics\\n\\nValidation set accuracy: 0.7113\\n\\nTest set accuracy: 0.7116\\n\\nLimitations\\n\\nData Bias: The model's performance may be influenced by biases present in the training data.\\n\\nGeneralization: The model's ability to generalize to unseen data is subject to the diversity of the training dataset.\"),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/detr-resnet-50.md'}, page_content='license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport\\n\\nDETR (End-to-End Object Detection) model with ResNet-50 backbone\\n\\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\\n\\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100.\\n\\nThe model is trained using a \"bipartite matching loss\": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a \"no object\" as class and \"no bounding box\" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for object detection. See the model hub to look for all available DETR models.\\n\\nHow to use\\n\\nHere is how to use this model:\\n\\n```python from transformers import DetrImageProcessor, DetrForObjectDetection import torch from PIL import Image import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nyou can specify the revision tag if you don\\'t want the timm dependency\\n\\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\") model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\\n\\ninputs = processor(images=image, return_tensors=\"pt\") outputs = model(**inputs)\\n\\nconvert outputs (bounding boxes and class logits) to COCO API\\n\\nlet\\'s only keep detections with score > 0.9\\n\\ntarget_sizes = torch.tensor([image.size[::-1]]) results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\\n\\nfor score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]): box = [round(i, 2) for i in box.tolist()] print( f\"Detected {model.config.id2label[label.item()]} with confidence \" f\"{round(score.item(), 3)} at location {box}\" ) This should output: Detected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98] Detected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66] Detected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76] Detected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93] Detected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72] ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe DETR model was trained on COCO 2017 object detection, a dataset consisting of 118k/5k annotated images for training/validation respectively.\\n\\nTraining procedure\\n\\nPreprocessing\\n\\nThe exact details of preprocessing of images during training/validation can be found here.\\n\\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\\n\\nTraining\\n\\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\\n\\nEvaluation results\\n\\nThis model achieves an AP (average precision) of 42.0 on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2005-12872, author = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko}, title = {End-to-End Object Detection with Transformers}, journal = {CoRR}, volume = {abs/2005.12872}, year = {2020}, url = {https://arxiv.org/abs/2005.12872}, archivePrefix = {arXiv}, eprint = {2005.12872}, timestamp = {Thu, 28 May 2020 17:38:09 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/sd-x2-latent-upscaler.md'}, page_content='license: openrail++ tags: - stable-diffusion inference: false\\n\\nStable Diffusion x2 latent upscaler model card\\n\\nThis model card focuses on the latent diffusion-based upscaler developed by Katherine Crowson in collaboration with Stability AI. This model was trained on a high-resolution subset of the LAION-2B dataset. It is a diffusion model that operates in the same latent space as the Stable Diffusion model, which is decoded into a full-resolution image. To use it with Stable Diffusion, You can take the generated latent from Stable Diffusion and pass it into the upscaler before decoding with your standard VAE. Or you can take any image, encode it into the latent space, use the upscaler, and decode it.\\n\\nNote: This upscaling model is designed explicitely for Stable Diffusion as it can upscale Stable Diffusion\\'s latent denoised image embeddings. This allows for very fast text-to-image + upscaling pipelines as all intermeditate states can be kept on GPU. More for information, see example below. This model works on all Stable Diffusion checkpoints\\n\\nImage by Tanishq Abraham from Stability AI originating from this tweet\\n\\nOriginal output image 2x upscaled output image\\n\\nUse it with üß® diffusers\\n\\nModel Details\\n\\nDeveloped by: Katherine Crowson\\n\\nModel type: Diffusion-based latent upscaler\\n\\nLanguage(s): English\\n\\nLicense: CreativeML Open RAIL++-M License\\n\\nExamples\\n\\nUsing the ü§ó\\'s Diffusers library to run latent upscaler on top of any StableDiffusionUpscalePipeline checkpoint to enhance its output image resolution by a factor of 2.\\n\\nbash pip install git+https://github.com/huggingface/diffusers.git pip install transformers accelerate scipy safetensors\\n\\n```python from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline import torch\\n\\npipeline = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16) pipeline.to(\"cuda\")\\n\\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(\"stabilityai/sd-x2-latent-upscaler\", torch_dtype=torch.float16) upscaler.to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut high resolution, unreal engine, ultra realistic\" generator = torch.manual_seed(33)\\n\\nwe stay in latent space! Let\\'s make sure that Stable Diffusion returns the image\\n\\nin latent space\\n\\nlow_res_latents = pipeline(prompt, generator=generator, output_type=\"latent\").images\\n\\nupscaled_image = upscaler( prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator, ).images[0]\\n\\nLet\\'s save the upscaled image under \"upscaled_astronaut.png\"\\n\\nupscaled_image.save(\"astronaut_1024.png\")\\n\\nas a comparison: Let\\'s also save the low-res image\\n\\nwith torch.no_grad(): image = pipeline.decode_latents(low_res_latents) image = pipeline.numpy_to_pil(image)[0]\\n\\nimage.save(\"astronaut_512.png\") ```\\n\\nResult:\\n\\n512-res Astronaut\\n\\n1024-res Astronaut\\n\\nNotes: - Despite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance) - If you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\\n\\nUses\\n\\nDirect Use\\n\\nThe model is intended for research purposes only. Possible research areas and tasks include\\n\\nSafe deployment of models which have the potential to generate harmful content.\\n\\nProbing and understanding the limitations and biases of generative models.\\n\\nGeneration of artworks and use in design and other artistic processes.\\n\\nApplications in educational or creative tools.\\n\\nResearch on generative models.\\n\\nExcluded uses are described below.\\n\\n### Misuse, Malicious Use, and Out-of-Scope Use Note: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\\n\\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\\n\\nOut-of-Scope Use\\n\\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\nMisuse and Malicious Use\\n\\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\\n\\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\\n\\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\\n\\nImpersonating individuals without their consent.\\n\\nSexual content without consent of the people who might see it.\\n\\nMis- and disinformation\\n\\nRepresentations of egregious violence and gore\\n\\nSharing of copyrighted or licensed material in violation of its terms of use.\\n\\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\\n\\nLimitations and Bias\\n\\nLimitations\\n\\nThe model does not achieve perfect photorealism\\n\\nThe model cannot render legible text\\n\\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\\n\\nFaces and people in general may not be generated properly.\\n\\nThe model was trained mainly with English captions and will not work as well in other languages.\\n\\nThe autoencoding part of the model is lossy\\n\\nThe model was trained on a subset of the large-scale dataset LAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION\\'s NFSW detector (see Training section).\\n\\nBias\\n\\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. Stable Diffusion vw was primarily trained on subsets of LAION-2B(en), which consists of images that are limited to English descriptions. Texts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. This affects the overall output of the model, as white and western cultures are often set as the default. Further, the ability of the model to generate content with non-English prompts is significantly worse than with English-language prompts. Stable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/mobilevit-small.md'}, page_content='license: other tags: - vision - image-classification datasets: - imagenet-1k widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace\\n\\nMobileViT (small-sized model)\\n\\nMobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.\\n\\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nMobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are \"unflattened\" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\\n\\n```python from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification from PIL import Image import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nfeature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-small\") model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\")\\n\\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\\n\\noutputs = model(**inputs) logits = outputs.logits\\n\\nmodel predicts one of the 1000 ImageNet classes\\n\\npredicted_class_idx = logits.argmax(-1).item() print(\"Predicted class:\", model.config.id2label[predicted_class_idx]) ```\\n\\nCurrently, both the feature extractor and model support PyTorch.\\n\\nTraining data\\n\\nThe MobileViT model was pretrained on ImageNet-1k, a dataset consisting of 1 million images and 1,000 classes.\\n\\nTraining procedure\\n\\nPreprocessing\\n\\nTraining requires only basic data augmentation, i.e. random resized cropping and horizontal flipping.\\n\\nTo learn multi-scale representations without requiring fine-tuning, a multi-scale sampler was used during training, with image sizes randomly sampled from: (160, 160), (192, 192), (256, 256), (288, 288), (320, 320).\\n\\nAt inference time, images are resized/rescaled to the same resolution (288x288), and center-cropped at 256x256.\\n\\nPixels are normalized to the range [0, 1]. Images are expected to be in BGR pixel order, not RGB.\\n\\nPretraining\\n\\nThe MobileViT networks are trained from scratch for 300 epochs on ImageNet-1k on 8 NVIDIA GPUs with an effective batch size of 1024 and learning rate warmup for 3k steps, followed by cosine annealing. Also used were label smoothing cross-entropy loss and L2 weight decay. Training resolution varies from 160x160 to 320x320, using multi-scale sampling.\\n\\nEvaluation results\\n\\nModel ImageNet top-1 accuracy ImageNet top-5 accuracy # params URL MobileViT-XXS 69.0 88.9 1.3 M https://huggingface.co/apple/mobilevit-xx-small MobileViT-XS 74.8 92.3 2.3 M https://huggingface.co/apple/mobilevit-x-small MobileViT-S 78.4 94.1 5.6 M https://huggingface.co/apple/mobilevit-small\\n\\nBibTeX entry and citation info\\n\\nbibtex @inproceedings{vision-transformer, title = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer}, author = {Sachin Mehta and Mohammad Rastegari}, year = {2022}, URL = {https://arxiv.org/abs/2110.02178} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/resnet-18.md'}, page_content='license: apache-2.0 tags: - vision - image-classification\\n\\ndatasets: - imagenet-1k\\n\\nwidget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace\\n\\nResNet\\n\\nResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository.\\n\\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\\n\\nIntended uses & limitations\\n\\nYou can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model:\\n\\n```python\\n\\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification import torch from datasets import load_dataset\\n\\ndataset = load_dataset(\"huggingface/cats-image\") image = dataset[\"test\"][\"image\"][0]\\n\\nimage_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-18\") model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-18\")\\n\\ninputs = image_processor(image, return_tensors=\"pt\")\\n\\nwith torch.no_grad(): ... logits = model(**inputs).logits\\n\\nmodel predicts one of the 1000 ImageNet classes\\n\\npredicted_label = logits.argmax(-1).item() print(model.config.id2label[predicted_label]) tiger cat ```\\n\\nFor more code examples, we refer to the documentation.'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/mit-b0.md'}, page_content='license: other tags: - vision datasets: - imagenet_1k widget: - src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg example_title: House - src: https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg example_title: Castle\\n\\nSegFormer (b0-sized) encoder pre-trained-only\\n\\nSegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\\n\\nDisclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.\\n\\nModel description\\n\\nSegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\\n\\nThis repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.\\n\\nIntended uses & limitations\\n\\nYou can use the model for fine-tuning of semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.\\n\\nHow to use\\n\\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\\n\\n```python from transformers import SegformerImageProcessor, SegformerForImageClassification from PIL import Image import requests\\n\\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\" image = Image.open(requests.get(url, stream=True).raw)\\n\\nimage_processor = SegformerImageProcessor.from_pretrained(\"nvidia/mit-b0\") model = SegformerForImageClassification.from_pretrained(\"nvidia/mit-b0\")\\n\\ninputs = image_processor(images=image, return_tensors=\"pt\") outputs = model(**inputs) logits = outputs.logits\\n\\nmodel predicts one of the 1000 ImageNet classes\\n\\npredicted_class_idx = logits.argmax(-1).item() print(\"Predicted class:\", model.config.id2label[predicted_class_idx]) ```\\n\\nFor more code examples, we refer to the documentation.\\n\\nLicense\\n\\nThe license for this model can be found here.\\n\\nBibTeX entry and citation info\\n\\nbibtex @article{DBLP:journals/corr/abs-2105-15203, author = {Enze Xie and Wenhai Wang and Zhiding Yu and Anima Anandkumar and Jose M. Alvarez and Ping Luo}, title = {SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}, journal = {CoRR}, volume = {abs/2105.15203}, year = {2021}, url = {https://arxiv.org/abs/2105.15203}, eprinttype = {arXiv}, eprint = {2105.15203}, timestamp = {Wed, 02 Jun 2021 11:46:42 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib}, bibsource = {dblp computer science bibliography, https://dblp.org} }'),\n",
       " Document(metadata={'source': '/workspaces/composition-blueprint-engine/sdi/services/yolov10s.md'}, page_content='license: agpl-3.0 tags: - object-detection - computer-vision - yolov10 - pytorch_model_hub_mixin datasets: - detection-datasets/coco library_name: yolov10 inference: false\\n\\nModel Description\\n\\nYOLOv10: Real-Time End-to-End Object Detection\\n\\narXiv: https://arxiv.org/abs/2405.14458v1\\n\\ngithub: https://github.com/THU-MIG/yolov10\\n\\nInstallation\\n\\npip install git+https://github.com/THU-MIG/yolov10.git\\n\\nTraining and validation\\n\\n```python from ultralytics import YOLOv10\\n\\nmodel = YOLOv10.from_pretrained(\\'jameslahm/yolov10s\\')\\n\\nTraining\\n\\nmodel.train(...)\\n\\nafter training, one can push to the hub\\n\\nmodel.push_to_hub(\"your-hf-username/yolov10-finetuned\")\\n\\nValidation\\n\\nmodel.val(...) ```\\n\\nInference\\n\\nHere\\'s an end-to-end example showcasing inference on a cats image:\\n\\n```python from ultralytics import YOLOv10\\n\\nmodel = YOLOv10.from_pretrained(\\'jameslahm/yolov10s\\') source = \\'http://images.cocodataset.org/val2017/000000039769.jpg\\' model.predict(source=source, save=True) ``` which shows:\\n\\nBibTeX Entry and Citation Info\\n\\n@article{wang2024yolov10, title={YOLOv10: Real-Time End-to-End Object Detection}, author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang}, journal={arXiv preprint arXiv:2405.14458}, year={2024} }')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_dir = \"/workspaces/composition-blueprint-engine/sdi/services\"\n",
    "documents = load_documents_from_markdown(service_dir)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_vectorstore(documents, persist_directory):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x7f8beff10fd0>\n"
     ]
    }
   ],
   "source": [
    "persist_dir = \"./chroma_local_db\"\n",
    "vectorstore = create_chroma_vectorstore(documents, persist_directory=persist_dir)\n",
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def create_rag_chain(retriever, prompt, llm_model):\n",
    "    llm = ChatOpenAI(model_name=llm_model, temperature=0)\n",
    "    return {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toml_filepath = \"prompts.toml\"\n",
    "with open(toml_filepath, \"rb\") as file:\n",
    "    prompts = tomllib.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_dir = \"./services\"\n",
    "toml_file = \"prompts.toml\"\n",
    "persist_dir = \"./chroma_local_db\"\n",
    "requirements_dir = \"./requirements/turtlebot-emergency.md\"\n",
    "question = load_md_file(requirements_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"You are an AI assistant specializing in decomposing user requirements into tasks that leverage available services. \"\n",
    "    \"Using the provided context, identify the relevant services and decompose the request into tasks. \"\n",
    "    \"When a relevant service is available, use its exact name in the `task` field; if multiple services match, prioritize the most specific or contextually appropriate service. \"\n",
    "    \"Only use a descriptive task name if no service matches. \"\n",
    "    \"For each task, ensure it has a unique ID, dependencies (`dep`), and arguments (`args`). \"\n",
    "    \"Base the response strictly on available services and contextual requirements. If a service is partially relevant, adapt its use case where possible. \"\n",
    "    \"Do not include additional explanations or return an empty list unless no mapping is possible for all requirements. \"\n",
    "    \"Given the user requirements, you must provide multiple valid breakdowns for the system in JSON format. Each breakdown should explore different combinations of services or workflows, where possible. Ensure that each JSON is a standalone solution.\"\n",
    "    \"Return them in the format below: \"\n",
    "    \"```json \"\n",
    "    \"[JSON 1]\"\n",
    "    \"[JSON 2]\"\n",
    "    \"and so on\"\n",
    "    \"Example 1: \"\n",
    "    \"Question: Can you tell me how many objects are in e1.jpg? \"\n",
    "    \"Answer: [\"\n",
    "    \"{{\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"image\\\": \\\"e1.jpg\\\"}}}}\"\n",
    "    \"] \"\n",
    "    \"Example 2: \"\n",
    "    \"Question: In e2.jpg, what‚Äôs the animal and what‚Äôs it doing? \"\n",
    "    \"Answer: [\"\n",
    "    \"{{\\\"task\\\": \\\"image-to-text\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"image\\\": \\\"e2.jpg\\\"}}}}, \"\n",
    "    \"{{\\\"task\\\": \\\"image-cls\\\", \\\"id\\\": 1, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"image\\\": \\\"e2.jpg\\\"}}}}, \"\n",
    "    \"{{\\\"task\\\": \\\"object-detection\\\", \\\"id\\\": 2, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"image\\\": \\\"e2.jpg\\\"}}}}, \"\n",
    "    \"{{\\\"task\\\": \\\"visual-question-answering\\\", \\\"id\\\": 3, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"text\\\": \\\"what‚Äôs the animal doing?\\\", \\\"image\\\": \\\"e2.jpg\\\"}}}}\"\n",
    "    \"] \"\n",
    "    \"Example 3: \"\n",
    "    \"Question: First generate a HED image of e3.jpg, then based on the HED image and a text ‚Äúa girl reading a book,‚Äù create a new image as a response. \"\n",
    "    \"Answer: [\"\n",
    "    \"{{\\\"task\\\": \\\"pose-detection\\\", \\\"id\\\": 0, \\\"dep\\\": [-1], \\\"args\\\": {{\\\"image\\\": \\\"e3.jpg\\\"}}}}, \"\n",
    "    \"{{\\\"task\\\": \\\"pose-text-to-image\\\", \\\"id\\\": 1, \\\"dep\\\": [0], \\\"args\\\": {{\\\"text\\\": \\\"a girl reading a book\\\", \\\"image\\\": \\\"<resource>-0\\\"}}}}\"\n",
    "    \"] \"\n",
    "    \"Now, based on the above examples, decompose the following user request into structured tasks that follow the json structure provided in the examples: \"\n",
    "    \"Question: {question} \"\n",
    "    \"{context} \"\n",
    "    \"Helpful Answer: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_breakdowns(json_data, directory=\"output_jsons\"):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Iterate over the JSON breakdowns\n",
    "    for idx, breakdown in enumerate(json_data):\n",
    "        file_path = os.path.join(directory, f\"breakdown_{idx + 1}.json\")\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(breakdown, json_file, indent=4)\n",
    "        print(f\"Saved breakdown {idx + 1} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved breakdown 1 to output_jsons/breakdown_1.json\n",
      "Saved breakdown 2 to output_jsons/breakdown_2.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'task': 'object-detection',\n",
       "  'id': 0,\n",
       "  'dep': [-1],\n",
       "  'args': {'image': '/camera/image_raw'}},\n",
       " {'task': 'path-planning',\n",
       "  'id': 1,\n",
       "  'dep': [0],\n",
       "  'args': {'detection_results': '<resource>-0', 'output_topic': '/cmd_vel'}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "# Prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# template = prompts[\"description\"]\n",
    "\n",
    "# template = (\"You are an AI assistant, expert at requirement decomposition and service composition. \" \n",
    "# \"You are provided with various services that might be fit for fulfilling the user's request. \"\n",
    "# \"Your job is to break down the user's request and select the appropriate services that will fulfill the decomposed tasks. \"\n",
    "# \"If there are not suitable services available to fit the user's requirements, say that it is not possible to do so. \"\n",
    "# \"You should give your answer in a structure json output with clear indication of tasks, selected services, and any dependencies (file, values) between these tasks. \"\n",
    "# \"Question: {question}\"\n",
    "# \"{context}\"\n",
    "# \"Helpful Answer: \"\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JsonOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "result = rag_chain.invoke(\n",
    "    question\n",
    ")\n",
    "save_json_breakdowns(result)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
